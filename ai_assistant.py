# -*- coding: utf-8 -*-
"""Copy of Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D4kEyQUBUx3iVz9PvMaxS0VCS5wclc57
"""

!pip install optimum[openvino]
!pip install gradio diffusers optimum torchaudio moviepy requests serpapi
!pip install google-search-results transformers accelerate yt_dlp

import gradio as gr
import torch
import torchaudio
import os
import warnings
from moviepy.editor import VideoFileClip
from transformers import pipeline, AutoProcessor, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM
from optimum.intel.openvino import OVModelForSpeechSeq2Seq, OVModelForSeq2SeqLM, OVModelForCausalLM
from io import BytesIO
from serpapi import GoogleSearch
import numpy as np
import gc
import time
import requests
from PIL import Image
import re
import yt_dlp
import tempfile
from urllib.parse import urlparse, parse_qs

warnings.filterwarnings("ignore", category=UserWarning)

UPLOAD_FOLDER = "uploads"
MODEL_CACHE = "model_cache"
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(MODEL_CACHE, exist_ok=True)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üîß Using device: {device}")

whisper_model = None
whisper_processor = None
notes_model = None
notes_tokenizer = None
chatbot_model = None
chatbot_tokenizer = None
chat_history = []

def load_whisper_model():
    global whisper_model, whisper_processor
    if whisper_model is None:
        print("üîÑ Loading Whisper Small model...")
        whisper_model_path = os.path.join(MODEL_CACHE, "whisper-small-ov")
        if not os.path.exists(whisper_model_path):
            print("üîÑ Converting Whisper to OpenVINO...")
            whisper_model = OVModelForSpeechSeq2Seq.from_pretrained("openai/whisper-small", export=True)
            whisper_model.save_pretrained(whisper_model_path)
        else:
            whisper_model = OVModelForSpeechSeq2Seq.from_pretrained(whisper_model_path)
        whisper_processor = AutoProcessor.from_pretrained("openai/whisper-small")
        print("‚úÖ Whisper model loaded!")

def load_notes_model():
    global notes_model, notes_tokenizer
    if notes_model is None:
        print("üîÑ Loading Qwen2.5-0.5B for notes generation...")
        notes_model_path = os.path.join(MODEL_CACHE, "qwen2.5-0.5b-ov")
        try:
            if not os.path.exists(notes_model_path):
                print("üîÑ Converting Qwen2.5-0.5B to OpenVINO...")
                notes_model = OVModelForCausalLM.from_pretrained(
                    "Qwen/Qwen2.5-0.5B-Instruct",
                    export=True,
                    trust_remote_code=True
                )
                notes_model.save_pretrained(notes_model_path)
            else:
                notes_model = OVModelForCausalLM.from_pretrained(
                    notes_model_path,
                    trust_remote_code=True
                )

            notes_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct", trust_remote_code=True)

            if notes_tokenizer.pad_token is None:
                notes_tokenizer.pad_token = notes_tokenizer.eos_token

            print("‚úÖ Qwen2.5-0.5B model loaded successfully!")

        except Exception as e:
            print(f"‚ö†Ô∏è Qwen2.5 model loading failed: {e}")
            notes_model = None
            notes_tokenizer = None

def load_chatbot_model():
    """Load a lightweight chatbot model for Q&A"""
    global chatbot_model, chatbot_tokenizer
    if chatbot_model is None:
        print("üîÑ Loading chatbot model...")
        chatbot_model_path = os.path.join(MODEL_CACHE, "chatbot-model-ov")
        try:
            if not os.path.exists(chatbot_model_path):
                print("üîÑ Converting chatbot model to OpenVINO...")
                chatbot_model = OVModelForCausalLM.from_pretrained(
                    "Qwen/Qwen2.5-0.5B-Instruct",
                    export=True,
                    trust_remote_code=True
                )
                chatbot_model.save_pretrained(chatbot_model_path)
            else:
                chatbot_model = OVModelForCausalLM.from_pretrained(
                    chatbot_model_path,
                    trust_remote_code=True
                )

            chatbot_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-0.5B-Instruct", trust_remote_code=True)

            if chatbot_tokenizer.pad_token is None:
                chatbot_tokenizer.pad_token = chatbot_tokenizer.eos_token

            print("‚úÖ Chatbot model loaded successfully!")
        except Exception as e:
            print(f"‚ö†Ô∏è Chatbot model loading failed: {e}")
            chatbot_model = None
            chatbot_tokenizer = None

def generate_chatbot_response(message, history):
    """Generate chatbot response using the loaded model"""
    global chat_history

    if not message.strip():
        return history + [("", "Please ask me a question about your studies!")]

    load_chatbot_model()

    if chatbot_model is None or chatbot_tokenizer is None:
        return history + [(message, "Sorry, I'm having trouble loading the chatbot model. Please try again later.")]

    try:
        context = ""
        if len(history) > 0:
            recent_history = history[-2:]
            for user_msg, bot_msg in recent_history:
                if user_msg and bot_msg:
                    context += f"Human: {user_msg}\nAssistant: {bot_msg}\n"

        system_prompt = """You are a knowledgeable and friendly AI teaching assistant. Your goal is to help students learn effectively by:

- Providing complete, well-structured explanations
- Using clear, simple language appropriate for students
- Including relevant examples when helpful
- Breaking down complex concepts step by step
- Always finishing your thoughts completely
- Being encouraging and supportive

Always provide complete answers and end with proper conclusions. Make sure every response is a complete thought that fully addresses the student's question."""

        full_prompt = f"""<|im_start|>system
{system_prompt}
<|im_end|>
"""

        if context:
            full_prompt += f"{context}"

        full_prompt += f"""<|im_start|>user
{message}
<|im_end|>
<|im_start|>assistant
"""

        inputs = chatbot_tokenizer(
            full_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=900,
            padding=True
        )

        with torch.no_grad():
            outputs = chatbot_model.generate(
                **inputs,
                max_new_tokens=1000,
                min_new_tokens=20,
                temperature=0.6,
                do_sample=True,
                top_p=0.85,
                top_k=40,
                pad_token_id=chatbot_tokenizer.pad_token_id,
                eos_token_id=chatbot_tokenizer.eos_token_id,
                early_stopping=False,
                no_repeat_ngram_size=3,
                repetition_penalty=1.15,
                length_penalty=1.0,
                num_return_sequences=1
            )

        full_response = chatbot_tokenizer.decode(outputs[0], skip_special_tokens=True)

        if "<|im_start|>assistant" in full_response:
            bot_response = full_response.split("<|im_start|>assistant")[-1].strip()
        else:
            prompt_length = len(chatbot_tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True))
            bot_response = full_response[prompt_length:].strip()

        bot_response = enhance_chatbot_response(bot_response)

        if not bot_response or len(bot_response.split()) < 5:
            bot_response = generate_fallback_response(message)

        del inputs, outputs
        cleanup_memory()

        new_history = history + [(message, bot_response)]
        return new_history

    except Exception as e:
        error_response = "I apologize, but I encountered an error while processing your question. Please try asking again or rephrase your question, and I'll do my best to help!"
        return history + [(message, error_response)]

def enhance_chatbot_response(response):
    """Enhanced cleaning and completion of chatbot response"""
    try:
        response = re.sub(r'<\|.*?\|>', '', response)
        response = re.sub(r'<.*?>', '', response)
        response = re.sub(r'\|.*?\|', '', response)

        response = re.sub(r'^(Assistant:|AI:|Bot:|Response:)\s*', '', response, flags=re.IGNORECASE)

        response = re.sub(r'\s+', ' ', response).strip()

        sentences = re.split(r'(?<=[.!?])\s+', response)
        cleaned_sentences = []

        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 5:
                if sentence and sentence[0].islower():
                    sentence = sentence[0].upper() + sentence[1:]
                cleaned_sentences.append(sentence)

        cleaned_response = ' '.join(cleaned_sentences)

        if cleaned_response and not cleaned_response.endswith(('.', '!', '?', ':')):
            if cleaned_response.endswith(',') or cleaned_response.endswith(' and') or cleaned_response.endswith(' or'):
                cleaned_response = cleaned_response.rstrip(', ') + '.'
            else:
                cleaned_response += '.'

        sentences = cleaned_response.split('. ')
        unique_sentences = []
        for sentence in sentences:
            if sentence not in unique_sentences and len(sentence.strip()) > 3:
                unique_sentences.append(sentence)

        final_response = '. '.join(unique_sentences)

        if not final_response.endswith(('.', '!', '?')):
            final_response += '.'

        return final_response if final_response and len(final_response.split()) >= 3 else response

    except Exception as e:
        return response

def generate_fallback_response(question):
    """Generate a fallback response when the model fails"""
    question_lower = question.lower()

    if any(word in question_lower for word in ['math', 'mathematics', 'algebra', 'calculus', 'geometry']):
        return "I'd be happy to help with your math question! Mathematics involves logical problem-solving and step-by-step thinking. Could you please provide more specific details about the concept or problem you're working on? I can then break it down into manageable steps."

    elif any(word in question_lower for word in ['science', 'physics', 'chemistry', 'biology']):
        return "Science is all about understanding how the world works! I'm here to help explain scientific concepts in simple terms. Could you please specify which area of science you're asking about, or rephrase your question? I'll do my best to provide a clear explanation."

    elif any(word in question_lower for word in ['history', 'historical', 'past', 'timeline']):
        return "History helps us understand the past and learn from it! I can help explain historical events, their causes and effects, and their significance. Please provide more details about the specific historical topic or period you're interested in."

    elif any(word in question_lower for word in ['english', 'literature', 'writing', 'grammar']):
        return "Language and literature are powerful tools for communication and expression! I can help with grammar, writing techniques, literary analysis, and more. What specific aspect of English or literature would you like help with?"

    else:
        return "I understand you have a question about your studies. I'm here to help explain concepts, provide examples, and support your learning journey. Could you please rephrase your question or provide a bit more context? This will help me give you a more detailed and helpful answer."

def clear_chat_history():
    """Clear the chat history"""
    return []

def is_youtube_url(url):
    """Check if the provided URL is a valid YouTube URL"""
    if not url:
        return False

    youtube_patterns = [
        r'(?:https?://)?(?:www\.)?youtube\.com/watch\?v=([a-zA-Z0-9_-]+)',
        r'(?:https?://)?(?:www\.)?youtu\.be/([a-zA-Z0-9_-]+)',
        r'(?:https?://)?(?:www\.)?youtube\.com/embed/([a-zA-Z0-9_-]+)',
        r'(?:https?://)?(?:www\.)?youtube\.com/v/([a-zA-Z0-9_-]+)'
    ]

    for pattern in youtube_patterns:
        if re.search(pattern, url):
            return True
    return False

def download_youtube_video(url, progress_callback=None, max_retries=3):
    """
    Download YouTube video with robust error handling and retry logic
    """
    try:
        if not is_youtube_url(url):
            return None, "‚ùå Invalid YouTube URL provided"
        temp_dir = tempfile.mkdtemp()

        ydl_opts = {

            'format': 'worst[height>=360][ext=mp4]/worst[ext=mp4]/worst',
            'outtmpl': os.path.join(temp_dir, '%(title).50s.%(ext)s'),
            'writesubtitles': False,
            'writeautomaticsub': False,
            'extract_flat': False,
            'no_warnings': True,
            'quiet': True,
            'socket_timeout': 30,
            'retries': 3,
            'fragment_retries': 3,
            'http_chunk_size': 1048576,
            'http_headers': {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
        }


        if progress_callback:
            def progress_hook(d):
                try:
                    if d['status'] == 'downloading':
                        if '_percent_str' in d:
                            percent_str = d['_percent_str'].replace('%', '')
                            percent = float(percent_str)
                            progress_callback(percent / 100, f"üì• Downloading: {percent:.1f}%")
                        else:
                            progress_callback(0.5, "üì• Downloading YouTube video...")
                    elif d['status'] == 'finished':
                        progress_callback(1.0, "‚úÖ Download complete!")
                except:
                    progress_callback(0.5, "üì• Downloading...")

            ydl_opts['progress_hooks'] = [progress_hook]

        for attempt in range(max_retries):
            try:
                if progress_callback:
                    progress_callback(0.05, f"üîÑ Attempt {attempt + 1}/{max_retries}...")

                with yt_dlp.YoutubeDL(ydl_opts) as ydl:

                    if progress_callback:
                        progress_callback(0.1, "üîç Extracting video info...")

                    info = ydl.extract_info(url, download=False)
                    if not info:
                        return None, "‚ùå Could not extract video information"

                    video_title = info.get('title', 'Unknown')[:50]
                    duration = info.get('duration', 0)

                    if duration and duration > 7200:
                        return None, f"‚ùå Video too long ({duration//60} minutes). Please use videos under 2 hours."

                    if progress_callback:
                        progress_callback(0.15, f"üì∫ Found: {video_title}...")

                    ydl.download([url])

                    downloaded_files = [f for f in os.listdir(temp_dir)
                                     if os.path.isfile(os.path.join(temp_dir, f)) and not f.startswith('.')]

                    if downloaded_files:
                        video_path = os.path.join(temp_dir, downloaded_files[0])

                        if os.path.exists(video_path) and os.path.getsize(video_path) > 0:
                            return video_path, f"‚úÖ Downloaded: {video_title}"
                        else:
                            raise Exception("Downloaded file is empty or corrupted")
                    else:
                        raise Exception("No files found after download")

            except yt_dlp.utils.DownloadError as e:
                error_msg = str(e)
                if "HTTP Error 429" in error_msg:
                    if attempt < max_retries - 1:
                        if progress_callback:
                            progress_callback(0.3, f"‚è≥ Rate limited, waiting before retry {attempt + 2}...")
                        time.sleep(5)
                        continue
                    else:
                        return None, "‚ùå YouTube rate limit exceeded. Please try again later."
                elif "timeout" in error_msg.lower():
                    if attempt < max_retries - 1:
                        if progress_callback:
                            progress_callback(0.3, f"‚è≥ Connection timeout, retrying {attempt + 2}...")
                        time.sleep(2)
                        continue
                    else:
                        return None, "‚ùå Connection timeout. Please check your internet connection and try again."
                else:
                    if attempt < max_retries - 1:
                        if progress_callback:
                            progress_callback(0.3, f"‚è≥ Download error, retrying {attempt + 2}...")
                        time.sleep(2)
                        continue
                    else:
                        return None, f"‚ùå Download failed after {max_retries} attempts: {error_msg}"

            except Exception as e:
                if attempt < max_retries - 1:
                    if progress_callback:
                        progress_callback(0.3, f"‚è≥ Error occurred, retrying {attempt + 2}...")
                    time.sleep(2)
                    continue
                else:
                    return None, f"‚ùå Download failed after {max_retries} attempts: {str(e)}"

        return None, f"‚ùå Download failed after {max_retries} attempts"

    except Exception as e:
        return None, f"‚ùå YouTube download error: {str(e)}"

def download_youtube_audio_only(url, progress_callback=None):
    """
    Download only audio from YouTube video (more reliable for transcription)
    """
    try:
        if not is_youtube_url(url):
            return None, "‚ùå Invalid YouTube URL provided"

        temp_dir = tempfile.mkdtemp()

        ydl_opts = {
            'format': 'bestaudio[ext=m4a]/bestaudio/best',
            'outtmpl': os.path.join(temp_dir, '%(title).50s.%(ext)s'),
            'writesubtitles': False,
            'writeautomaticsub': False,
            'extract_flat': False,
            'no_warnings': True,
            'quiet': True,
            'socket_timeout': 30,
            'retries': 5,
            'fragment_retries': 5,
            'prefer_ffmpeg': True,
            'keepvideo': False,
            'postprocessors': [{
              'key': 'FFmpegExtractAudio',
              'preferredcodec': 'mp3',
              'preferredquality': '192',
            }],
            'http_headers': {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
        }

        if progress_callback:
            def progress_hook(d):
                try:
                    if d['status'] == 'downloading':
                        if '_percent_str' in d:
                            percent = float(d['_percent_str'].replace('%', ''))
                            progress_callback(percent / 100, f"üéµ Downloading audio: {percent:.1f}%")
                        else:
                            progress_callback(0.5, "üéµ Downloading audio...")
                    elif d['status'] == 'finished':
                        progress_callback(1.0, "‚úÖ Audio download complete!")
                except:
                    progress_callback(0.5, "üéµ Downloading...")

            ydl_opts['progress_hooks'] = [progress_hook]

        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            if progress_callback:
                progress_callback(0.1, "üîç Extracting video info...")

            info = ydl.extract_info(url, download=False)
            if not info:
                return None, "‚ùå Could not extract video information"

            video_title = info.get('title', 'Unknown')[:50]
            duration = info.get('duration', 0)

            if duration and duration > 7200:
                return None, f"‚ùå Video too long ({duration//60} minutes). Please use videos under 2 hours."

            if progress_callback:
                progress_callback(0.15, f"üéµ Downloading audio for: {video_title}...")

            ydl.download([url])

            downloaded_files = [f for f in os.listdir(temp_dir)
                             if os.path.isfile(os.path.join(temp_dir, f)) and not f.startswith('.')]

            if downloaded_files:
                audio_path = os.path.join(temp_dir, downloaded_files[0])
                if os.path.exists(audio_path) and os.path.getsize(audio_path) > 0:
                    return audio_path, f"‚úÖ Downloaded audio: {video_title}"
                else:
                    return None, "‚ùå Downloaded file is empty"
            else:
                return None, "‚ùå No audio file found after download"

    except Exception as e:
        return None, f"‚ùå Audio download error: {str(e)}"

def cleanup_memory():
    """Enhanced memory cleanup function"""
    try:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

        import gc
        gc.collect()

        if 'torch' in globals():
            for obj_name in list(globals().keys()):
                obj = globals()[obj_name]
                if hasattr(obj, 'cpu'):
                    try:
                        obj.cpu()
                        del obj
                    except:
                        pass

    except Exception as e:
        print(f"Memory cleanup warning: {e}")

def transcribe_audio_from_video(video_file, progress=gr.Progress()):
    if not video_file:
        return "‚ùå No video uploaded."

    load_whisper_model()

    try:
        progress(0, desc="Extracting audio...")
        video = VideoFileClip(video_file)
        if not video.audio:
            video.close()
            return "‚ùå No audio found in video."

        audio_path = os.path.join(UPLOAD_FOLDER, "audio.wav")
        video.audio.write_audiofile(audio_path, logger=None)
        video.close()

        del video
        cleanup_memory()

        waveform, sample_rate = torchaudio.load(audio_path)
        if sample_rate != 16000:
            resample = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
            waveform = resample(waveform)
            sample_rate = 16000

        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)

        progress(0.2, desc="Processing audio chunks...")

        chunk_length_sec = 20
        chunk_size = chunk_length_sec * sample_rate
        stride = int(sample_rate * 4)
        transcripts = []

        total_length = waveform.size(1)
        batch_size = chunk_size * 3

        for batch_start in range(0, total_length, batch_size):
            batch_end = min(batch_start + batch_size, total_length)
            batch_waveform = waveform[:, batch_start:batch_end]

            for start in range(0, batch_waveform.size(1), chunk_size - stride):
                end = min(start + chunk_size, batch_waveform.size(1))
                chunk = batch_waveform[:, start:end]

                if chunk.size(1) < 8000:
                    continue

                try:
                    inputs = whisper_processor(chunk.squeeze(0), sampling_rate=16000, return_tensors="pt")

                    with torch.no_grad():
                        generated_ids = whisper_model.generate(inputs["input_features"])

                    text = whisper_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
                    if text and text.strip():
                        transcripts.append(text.strip())

                    del inputs, generated_ids

                except Exception as e:
                    print(f"‚ùå Chunk processing failed: {e}")
                    continue

            del batch_waveform
            cleanup_memory()

            progress_val = min(0.9, 0.2 + (batch_end / total_length) * 0.7)
            progress(progress_val, desc=f"Processing audio... {int((batch_end/total_length)*100)}%")

        if os.path.exists(audio_path):
            os.remove(audio_path)

        del waveform
        cleanup_memory()

        progress(1.0, desc="Transcription complete!")

        if not transcripts:
            return "‚ùå No speech detected in video. Try a video with clearer audio."

        final_transcript = " ".join(transcripts)
        return final_transcript if final_transcript.strip() else "‚ùå No clear speech detected."

    except Exception as e:
        cleanup_memory()
        return f"‚ùå Error processing video: {str(e)}"

import re
import torch
from collections import Counter

def generate_structured_notes(transcript):
    """Generate structured notes using Qwen2.5-0.5B model with fallback"""

    if notes_model is not None and notes_tokenizer is not None:
        try:
            return generate_notes_with_qwen(transcript)
        except Exception as e:
            print(f"‚ö†Ô∏è Qwen notes generation failed: {e}")
            cleanup_memory()

    if notes_model is None or notes_tokenizer is None:
        load_notes_model()
        if notes_model is not None and notes_tokenizer is not None:
            try:
                return generate_notes_with_qwen(transcript)
            except Exception as e:
                print(f"‚ö†Ô∏è Qwen notes generation failed after loading: {e}")
                cleanup_memory()

    return generate_notes_template_based(transcript)

def generate_notes_with_qwen(transcript):
    """Generate notes using Qwen2.5-0.5B model with improved prompting and processing"""
    try:
        max_chunk_length = 1200
        chunks = split_transcript_intelligently(transcript, max_chunk_length)

        all_notes = []

        for i, chunk in enumerate(chunks[:3]):
            try:
                chunk_notes = generate_chunk_notes(chunk, i == 0)
                if chunk_notes and len(chunk_notes.strip()) > 30:
                    all_notes.append(chunk_notes)

                cleanup_memory()

            except Exception as e:
                print(f"‚ö†Ô∏è Error processing chunk {i}: {e}")
                continue

        if all_notes:
            merged_notes = merge_chunk_notes(all_notes)
            return format_notes_as_text(merged_notes)
        else:
            return generate_notes_template_based(transcript)

    except Exception as e:
        print(f"‚ùå Qwen notes generation error: {str(e)}")
        cleanup_memory()
        return generate_notes_template_based(transcript)

def split_transcript_intelligently(transcript, max_length):
    """Split transcript at sentence boundaries to preserve context"""
    if len(transcript) <= max_length:
        return [transcript]

    chunks = []
    sentences = re.split(r'[.!?]+', transcript)
    current_chunk = ""

    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue

        if len(current_chunk) + len(sentence) + 2 <= max_length:
            current_chunk = current_chunk + ". " + sentence if current_chunk else sentence
        else:
            if current_chunk:
                chunks.append(current_chunk + ".")
            current_chunk = sentence

    if current_chunk:
        chunks.append(current_chunk + ".")

    return chunks

def generate_chunk_notes(chunk, is_first_chunk):
    """Generate notes for a single chunk with optimized prompting"""

    prompt = f"""<|im_start|>system
You are a note-taking assistant. Create clear, structured notes from lecture content. Use simple formatting and complete sentences.
<|im_end|>
<|im_start|>user
Create structured notes from this lecture content:

{chunk}

Format the notes with these sections:
- Key Points
- Important Terms
- Main Ideas
- Examples

Keep responses concise and factual.
<|im_end|>
<|im_start|>assistant
KEY POINTS:
"""

    try:
        inputs = notes_tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=1400,
            padding=False,
            return_attention_mask=True
        )

        with torch.no_grad():
            outputs = notes_model.generate(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                max_new_tokens=600,
                temperature=0.3,
                do_sample=True,
                top_p=0.7,
                top_k=30,
                pad_token_id=notes_tokenizer.pad_token_id,
                eos_token_id=notes_tokenizer.eos_token_id,
                early_stopping=True,
                no_repeat_ngram_size=4,
                repetition_penalty=1.2,
                length_penalty=1.1
            )

        response = notes_tokenizer.decode(outputs[0], skip_special_tokens=True)

        if "<|im_start|>assistant" in response:
            notes = response.split("<|im_start|>assistant")[-1].strip()
        else:
            notes = response[len(prompt):].strip()

        notes = clean_generated_notes_improved(notes)

        del inputs, outputs

        return notes if notes and len(notes.strip()) > 30 else ""

    except Exception as e:
        print(f"‚ùå Chunk generation error: {str(e)}")
        return ""

def clean_generated_notes_improved(notes):
    """Improved cleaning of generated notes"""
    try:
        notes = re.sub(r'<\|im_start\|>.*?<\|im_end\|>', '', notes, flags=re.DOTALL)
        notes = re.sub(r'<\|.*?\|>', '', notes)

        lines = notes.split('\n')
        cleaned_lines = []

        for line in lines:
            line = line.strip()
            if not line or len(line) < 5:
                continue

            if line.endswith(('...', '..', 'and', 'the', 'of', 'in', 'to')):
                continue

            line = re.sub(r'^[-‚Ä¢*]+\s*', '- ', line)
            line = re.sub(r'\s+', ' ', line)

            if line and line not in cleaned_lines:
                cleaned_lines.append(line)

        return '\n'.join(cleaned_lines)

    except Exception as e:
        return notes

def merge_chunk_notes(note_chunks):
    """Merge notes from multiple chunks intelligently"""
    merged = {
        'key_points': [],
        'important_terms': [],
        'main_ideas': [],
        'examples': []
    }

    for chunk_notes in note_chunks:
        parsed = parse_chunk_sections(chunk_notes)

        for section, items in parsed.items():
            if section in merged:
                merged[section].extend(items)

    for section in merged:
        merged[section] = remove_duplicates(merged[section])[:8]

    return merged

def parse_chunk_sections(notes):
    """Parse notes into different sections"""
    sections = {
        'key_points': [],
        'important_terms': [],
        'main_ideas': [],
        'examples': []
    }

    current_section = 'key_points'

    for line in notes.split('\n'):
        line = line.strip()
        if not line:
            continue

        line_lower = line.lower()

        if any(term in line_lower for term in ['key point', 'main point', 'important point']):
            current_section = 'key_points'
        elif any(term in line_lower for term in ['term', 'definition', 'concept']):
            current_section = 'important_terms'
        elif any(term in line_lower for term in ['main idea', 'central idea', 'core idea']):
            current_section = 'main_ideas'
        elif any(term in line_lower for term in ['example', 'instance', 'case']):
            current_section = 'examples'
        else:
            if line.startswith('-') or line.startswith('‚Ä¢'):
                line = line[1:].strip()

            if len(line) > 10:
                sections[current_section].append(line)

    return sections

def remove_duplicates(items):
    """Remove duplicate items while preserving order"""
    seen = set()
    result = []

    for item in items:
        item_key = item.lower().strip()[:50]
        if item_key not in seen:
            seen.add(item_key)
            result.append(item)

    return result

def format_notes_as_text(merged_notes):
    """Format merged notes as clean text without markdown"""
    output = []

    output.append("STRUCTURED LECTURE NOTES")
    output.append("=" * 50)
    output.append("")

    if merged_notes['key_points']:
        output.append("KEY POINTS:")
        for i, point in enumerate(merged_notes['key_points'], 1):
            output.append(f"{i}. {point}")
        output.append("")

    if merged_notes['important_terms']:
        output.append("IMPORTANT TERMS:")
        for i, term in enumerate(merged_notes['important_terms'], 1):
            output.append(f"{i}. {term}")
        output.append("")

    if merged_notes['main_ideas']:
        output.append("MAIN IDEAS:")
        for i, idea in enumerate(merged_notes['main_ideas'], 1):
            output.append(f"{i}. {idea}")
        output.append("")

    if merged_notes['examples']:
        output.append("EXAMPLES:")
        for i, example in enumerate(merged_notes['examples'], 1):
            output.append(f"{i}. {example}")
        output.append("")

    return '\n'.join(output)

def generate_notes_template_based(transcript):
    """Enhanced template-based notes generation with better structure"""
    try:
        cleaned_transcript = clean_transcript(transcript)
        concepts = extract_key_concepts_improved(cleaned_transcript)
        definitions = extract_definitions_improved(cleaned_transcript)
        processes = extract_processes_improved(cleaned_transcript)
        examples = extract_examples_improved(cleaned_transcript)

        summary = generate_summary_improved(cleaned_transcript, concepts)

        notes = []
        notes.append("STRUCTURED LECTURE NOTES")
        notes.append("=" * 50)
        notes.append("")

        if concepts:
            notes.append("KEY CONCEPTS:")
            for i, concept in enumerate(concepts[:8], 1):
                notes.append(f"{i}. {concept}")
            notes.append("")

        if definitions:
            notes.append("IMPORTANT DEFINITIONS:")
            for i, definition in enumerate(definitions[:6], 1):
                notes.append(f"{i}. {definition['term']}: {definition['definition']}")
            notes.append("")

        if processes:
            notes.append("METHODS AND PROCESSES:")
            for i, process in enumerate(processes[:6], 1):
                notes.append(f"{i}. {process}")
            notes.append("")

        if examples:
            notes.append("EXAMPLES AND APPLICATIONS:")
            for i, example in enumerate(examples[:6], 1):
                notes.append(f"{i}. {example}")
            notes.append("")

        notes.append("SUMMARY:")
        notes.append(summary)
        notes.append("")

        return '\n'.join(notes)

    except Exception as e:
        return f"Error generating notes: {str(e)}"

def clean_transcript(transcript):
    """Clean transcript for better processing"""
    text = re.sub(r'\b(um|uh|so|well|okay|alright|you know|like)\b', '', transcript, flags=re.IGNORECASE)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def extract_key_concepts_improved(text):
    """Improved concept extraction with better filtering"""
    concepts = []
    sentences = re.split(r'[.!?]+', text)

    concept_keywords = [
        'algorithm', 'method', 'technique', 'approach', 'model', 'theory',
        'principle', 'concept', 'framework', 'system', 'process', 'function',
        'learning', 'analysis', 'optimization', 'classification'
    ]

    concept_phrases = [
        'is a', 'are', 'refers to', 'defined as', 'known as', 'called',
        'this is', 'these are', 'we use', 'we can use'
    ]

    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 25:
            continue

        sentence_lower = sentence.lower()
        has_concept = any(keyword in sentence_lower for keyword in concept_keywords)
        has_description = any(phrase in sentence_lower for phrase in concept_phrases)

        if has_concept and has_description:
            cleaned = clean_sentence(sentence)
            if cleaned and len(cleaned) > 20:
                concepts.append(cleaned)

    return concepts

def extract_definitions_improved(text):
    """Extract definitions with better pattern matching"""
    definitions = []
    sentences = re.split(r'[.!?]+', text)

    patterns = [
        r'(\w+(?:\s+\w+){0,3})\s+(?:is|are|refers to|means|defined as|can be defined as)\s+(.+)',
        r'(?:the|a)\s+(\w+(?:\s+\w+){0,2})\s+(?:is|are)\s+(.+)',
        r'(\w+(?:\s+\w+){0,2})\s*:\s*(.+)',
        r'(\w+(?:\s+\w+){0,2})\s+(?:represents|indicates|shows|describes)\s+(.+)'
    ]

    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 30:
            continue

        for pattern in patterns:
            match = re.search(pattern, sentence, re.IGNORECASE)
            if match:
                term = match.group(1).strip().title()
                definition = match.group(2).strip()

                if len(definition) > 15 and len(term) < 40:
                    definitions.append({
                        'term': term,
                        'definition': definition
                    })
                break

    return definitions

def extract_processes_improved(text):
    """Extract processes and methods with better identification"""
    processes = []
    sentences = re.split(r'[.!?]+', text)

    process_indicators = [
        'step', 'first', 'then', 'next', 'finally', 'algorithm', 'procedure',
        'method', 'process', 'technique', 'approach', 'way to', 'how to',
        'we start', 'we begin', 'we compute', 'we calculate', 'we apply'
    ]

    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 30:
            continue

        sentence_lower = sentence.lower()
        if any(indicator in sentence_lower for indicator in process_indicators):
            cleaned = clean_sentence(sentence)
            if cleaned and len(cleaned) > 25:
                processes.append(cleaned)

    return processes

def extract_examples_improved(text):
    """Extract examples with better filtering"""
    examples = []
    sentences = re.split(r'[.!?]+', text)

    example_indicators = [
        'example', 'for instance', 'such as', 'like', 'including',
        'application', 'used in', 'used for', 'case study', 'consider',
        'suppose', 'imagine', 'let\'s say'
    ]

    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 25:
            continue

        sentence_lower = sentence.lower()
        if any(indicator in sentence_lower for indicator in example_indicators):
            cleaned = clean_sentence(sentence)
            if cleaned and len(cleaned) > 20:
                examples.append(cleaned)

    return examples

def generate_summary_improved(text, concepts):
    """Generate a more coherent summary"""
    words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())
    word_freq = Counter(words)

    common_words = {'this', 'that', 'with', 'have', 'will', 'from', 'they', 'been', 'have', 'were', 'said', 'each', 'which', 'their', 'time', 'what', 'about', 'when', 'where', 'would', 'there'}
    important_words = [word for word, freq in word_freq.most_common(10) if word not in common_words]

    subject_indicators = {
        'computer science': ['algorithm', 'programming', 'computer', 'software', 'code'],
        'mathematics': ['equation', 'theorem', 'proof', 'mathematical', 'formula'],
        'machine learning': ['learning', 'model', 'training', 'neural', 'data'],
        'physics': ['force', 'energy', 'particle', 'quantum', 'physics'],
        'biology': ['cell', 'organism', 'genetic', 'biology', 'species']
    }

    subject = "the topic"
    for area, keywords in subject_indicators.items():
        if any(keyword in text.lower() for keyword in keywords):
            subject = area
            break

    summary_parts = []
    summary_parts.append(f"This lecture covers fundamental aspects of {subject}.")

    if important_words:
        summary_parts.append(f"Key topics discussed include {', '.join(important_words[:3])}.")

    if concepts:
        summary_parts.append("The lecture presents important concepts, methodologies, and their practical applications.")

    summary_parts.append("The material provides essential knowledge for understanding this subject area.")

    return ' '.join(summary_parts)

def clean_sentence(sentence):
    """Clean and format individual sentences"""
    sentence = re.sub(r'\b(um|uh|so|well|okay|alright|you know)\b', '', sentence, flags=re.IGNORECASE)
    sentence = re.sub(r'\s+', ' ', sentence).strip()

    if sentence and len(sentence) > 5:
        sentence = sentence[0].upper() + sentence[1:]

        if not sentence.endswith(('.', '!', '?')):
            sentence += '.'

    return sentence

def search_google_images(query, num_images=2):
    if not query.strip():
        return []
    try:
        search = GoogleSearch({
            "q": query,
            "tbm": "isch",
            "api_key": "6afad91dbde40f2242975f874b68539d4f6828ce4c75d9d49719e42bd97896ec"
        })
        results = search.get_dict()
        if "error" in results:
            return []

        images_results = results.get("images_results", [])
        image_urls = []
        for img in images_results[:num_images]:
            original_url = img.get("original")
            if original_url:
                image_urls.append(original_url)
        return image_urls
    except Exception as e:
        print(f"‚ùå Image search error: {e}")
        return []

def download_image_from_url(url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code == 200:
            image = Image.open(BytesIO(response.content))
            return image.convert('RGB')
        return None
    except Exception as e:
        print(f"‚ùå Image download failed: {e}")
        return None

def search_web_enhanced(query, num_results=8, include_videos=True):
    if not query.strip():
        return "‚ùå No search query provided."
    try:
        search_params = {
            "q": query,
            "api_key": "6afad91dbde40f2242975f874b68539d4f6828ce4c75d9d49719e42bd97896ec",
            "num": num_results
        }
        search = GoogleSearch(search_params)
        results = search.get_dict()

        if "error" in results:
            return f"‚ùå Search API error: {results['error']}"

        organic_results = results.get("organic_results", [])
        formatted_results = []

        if organic_results:
            formatted_results.append("üîç **WEB RESULTS:**\n")
            for i, result in enumerate(organic_results[:num_results], 1):
                title = result.get("title", "No title")
                link = result.get("link", "")
                snippet = result.get("snippet", "No description available")
                result_text = f"{i}. **{title}**\n{snippet}\nüîó {link}\n"
                formatted_results.append(result_text)

        if include_videos:
            video_search = GoogleSearch({
                "q": query,
                "tbm": "vid",
                "api_key": "6afad91dbde40f2242975f874b68539d4f6828ce4c75d9d49719e42bd97896ec",
                "num": 5
            })
            video_results = video_search.get_dict()
            video_results_data = video_results.get("video_results", [])

            if video_results_data:
                formatted_results.append("\nüìπ **VIDEO RESULTS:**\n")
                for i, video in enumerate(video_results_data[:5], 1):
                    title = video.get("title", "No title")
                    link = video.get("link", "")
                    duration = video.get("duration", "")
                    channel = video.get("channel", "")
                    video_text = f"{i}. **{title}**"
                    if duration:
                        video_text += f" ({duration})"
                    if channel:
                        video_text += f" - {channel}"
                    video_text += f"\nüé• {link}\n"
                    formatted_results.append(video_text)

        return "\n".join(formatted_results) if formatted_results else "‚ùå No results found."
    except Exception as e:
        return f"‚ùå Search error: {str(e)}"

def extract_keywords_and_concepts(text, max_keywords=8):
    import re
    priority_terms = [
        'theory', 'concept', 'principle', 'method', 'process', 'system', 'model',
        'analysis', 'research', 'study', 'experiment', 'data', 'result', 'conclusion',
        'algorithm', 'function', 'equation', 'formula', 'definition', 'example',
        'application', 'implementation', 'solution', 'problem', 'approach'
    ]

    words = re.findall(r'\b[a-zA-Z]{4,}\b', text.lower())
    word_freq = {}
    for word in words:
        if len(word) >= 4:
            word_freq[word] = word_freq.get(word, 0) + 1

    for word in word_freq:
        if any(term in word for term in priority_terms):
            word_freq[word] *= 2

    top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:max_keywords]
    return [word for word, freq in top_keywords]

def create_search_query(transcript):
    keywords = extract_keywords_and_concepts(transcript[:500])
    if len(keywords) >= 3:
        return " ".join(keywords[:3]) + " tutorial explanation"
    return " ".join(keywords[:2]) + " educational content" if keywords else "educational tutorial"

def process_youtube_or_upload_robust(video_file, youtube_url, num_search_results=8, num_images=2, progress=gr.Progress()):
    """
    Enhanced pipeline with fallback to audio-only download for YouTube
    """
    video_path = None
    temp_file_path = None

    try:
        if youtube_url and youtube_url.strip():
            if not is_youtube_url(youtube_url.strip()):
                return "‚ùå Invalid YouTube URL format.", "", None, None, ""

            progress(0, desc="üîó Processing YouTube URL...")

            temp_file_path, download_msg = download_youtube_video(
                youtube_url.strip(),
                progress_callback=lambda p, desc: progress(p * 0.2, desc)
            )

            if not temp_file_path:
                progress(0.05, desc="üéµ Trying audio-only download...")
                temp_file_path, download_msg = download_youtube_audio_only(
                    youtube_url.strip(),
                    progress_callback=lambda p, desc: progress(0.05 + p * 0.25, desc)
                )

            if not temp_file_path:
                return download_msg, "", None, None, ""

            video_path = temp_file_path
            progress(0.3, desc="‚úÖ YouTube content ready for processing!")

        elif video_file:
            video_path = video_file
            progress(0.3, desc="‚úÖ Uploaded video ready for processing!")
        else:
            return "‚ùå Please either upload a video file or provide a YouTube URL.", "", None, None, ""

        progress(0.35, desc="üé¨ Starting transcription...")
        transcript = transcribe_audio_from_video(video_path, progress=lambda p, desc: progress(0.35 + p * 0.25, desc))
        if transcript.startswith("‚ùå"):
            return transcript, "", None, None, ""
        progress(0.6, desc="‚úÖ Transcription complete!")

        progress(0.65, desc="üìù Creating structured notes...")
        structured_notes = generate_structured_notes(transcript)
        progress(0.75, desc="‚úÖ Structured notes created!")

        progress(0.8, desc="üîç Analyzing content for search...")
        search_query = create_search_query(transcript)
        keywords = extract_keywords_and_concepts(transcript[:500])

        progress(0.85, desc="üåê Performing web search...")
        search_results = search_web_enhanced(search_query, num_search_results, include_videos=True)
        progress(0.9, desc="‚úÖ Search complete!")

        progress(0.95, desc="üñºÔ∏è Searching for relevant images...")
        image_query = " ".join(keywords[:4]) if len(keywords) >= 4 else search_query
        image_urls = search_google_images(image_query, num_images)

        images = []
        for i, url in enumerate(image_urls):
            img = download_image_from_url(url)
            images.append(img)

        image1 = images[0] if len(images) > 0 else None
        image2 = images[1] if len(images) > 1 else None

        progress(1.0, desc="‚úÖ Processing complete!")

        return transcript, structured_notes, image1, image2, search_results

    except Exception as e:
        error_msg = f"‚ùå Pipeline error: {str(e)}"
        return error_msg, "", None, None, ""

    finally:
        if temp_file_path and os.path.exists(temp_file_path):
            try:
                os.remove(temp_file_path)
                os.rmdir(os.path.dirname(temp_file_path))
            except:
                pass
        cleanup_memory()

custom_css = """
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');

:root {
    --primary-color-light: #007bff;
    --secondary-color-light: #6c757d;
    --bg-color-light: #ffffff;
    --text-color-light: #000000;
    --border-color-light: #dee2e6;

    --primary-color-dark: #0d6efd;
    --secondary-color-dark: #6c757d;
    --bg-color-dark: #1a1a1a;
    --text-color-dark: #ffffff;
    --border-color-dark: #404040;
}

/* Light mode styles */
.gradio-container {
    background-color: var(--bg-color-light);
    color: var(--text-color-light);
    font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
    font-weight: 400;
    line-height: 1.5;
}

/* Dark mode styles */
.dark .gradio-container {
    background-color: var(--bg-color-dark) !important;
    color: var(--text-color-dark) !important;
}

/* Input fields */
.gr-textbox, .gr-textbox textarea, .gr-textbox input,
textarea, input[type="text"] {
    background-color: var(--bg-color-light);
    color: var(--text-color-light);
    border: 1px solid var(--border-color-light);
    border-radius: 6px;
    font-family: 'Inter', sans-serif;
    font-size: 14px;
    transition: border-color 0.2s ease;
}

.gr-textbox:focus-within, .gr-textbox textarea:focus, .gr-textbox input:focus,
textarea:focus, input[type="text"]:focus {
    border-color: var(--primary-color-light);
    outline: none;
    box-shadow: 0 0 0 2px rgba(0, 123, 255, 0.1);
}

.dark .gr-textbox, .dark .gr-textbox textarea, .dark .gr-textbox input,
.dark textarea, .dark input[type="text"] {
    background-color: #2d2d2d !important;
    color: var(--text-color-dark) !important;
    border-color: var(--border-color-dark) !important;
}

.dark .gr-textbox:focus-within, .dark .gr-textbox textarea:focus, .dark .gr-textbox input:focus,
.dark textarea:focus, .dark input[type="text"]:focus {
    border-color: var(--primary-color-dark) !important;
    box-shadow: 0 0 0 2px rgba(13, 110, 253, 0.1) !important;
}

/* File upload areas */
.gr-file, .gr-video, .gr-image {
    background-color: var(--bg-color-light);
    border: 1px solid var(--border-color-light);
    border-radius: 6px;
    font-family: 'Inter', sans-serif;
}

.dark .gr-file, .dark .gr-video, .dark .gr-image {
    background-color: #2d2d2d !important;
    border-color: var(--border-color-dark) !important;
    color: var(--text-color-dark) !important;
}

/* Labels */
.gr-label, label {
    color: var(--text-color-light);
    font-weight: 500;
    font-family: 'Inter', sans-serif;
    font-size: 14px;
    margin-bottom: 6px;
}

.dark .gr-label, .dark label {
    color: var(--text-color-dark) !important;
}

/* Cards and panels */
.gr-panel, .gr-box, .gr-form {
    background-color: var(--bg-color-light);
    border: 1px solid var(--border-color-light);
    border-radius: 8px;
    margin: 8px 0;
    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.05);
}

.dark .gr-panel, .dark .gr-box, .dark .gr-form {
    background-color: #2d2d2d !important;
    border-color: var(--border-color-dark) !important;
    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.2) !important;
}

/* Buttons */
.gr-button {
    background: var(--primary-color-light);
    color: white;
    border: none;
    border-radius: 6px;
    padding: 10px 20px;
    font-weight: 500;
    font-family: 'Inter', sans-serif;
    font-size: 14px;
    transition: all 0.2s ease;
    cursor: pointer;
}

.gr-button:hover {
    background: #0056b3;
    transform: translateY(-1px);
    box-shadow: 0 2px 8px rgba(0, 123, 255, 0.2);
}

.dark .gr-button {
    background: var(--primary-color-dark);
}

.dark .gr-button:hover {
    background: #0b5ed7;
}

/* Tab navigation */
.tab-nav button {
    background: var(--primary-color-light);
    color: white;
    border: none;
    border-radius: 6px;
    padding: 10px 20px;
    margin: 2px;
    font-weight: 500;
    font-family: 'Inter', sans-serif;
    font-size: 14px;
    transition: all 0.2s ease;
}

.tab-nav button:hover {
    background: #0056b3;
}

.dark .tab-nav button {
    background: var(--primary-color-dark);
}

.dark .tab-nav button:hover {
    background: #0b5ed7;
}

/* Main header */
.main-header {
    text-align: center;
    font-size: 2.2em;
    font-weight: 600;
    margin-bottom: 24px;
    color: var(--primary-color-light);
    font-family: 'Inter', sans-serif;
    letter-spacing: -0.02em;
}

h1#main-title {
    font-family: 'Inter', sans-serif;
    font-weight: 600;
    font-size: 2.2rem;
    color: #1976d2;
    text-align: center;
    margin-bottom: 1rem;
    letter-spacing: -0.02em;
}

.dark .main-header {
    color: var(--primary-color-dark) !important;
}

/* Video section - full width */
.video-section {
    width: 100% !important;
    max-width: none !important;
}

.video-section .gr-video {
    width: 100% !important;
    min-height: 400px;
}

/* Results grid */
.results-grid {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 16px;
    margin-top: 16px;
}

.result-item {
    background: var(--bg-color-light);
    border: 1px solid var(--border-color-light);
    border-radius: 8px;
    padding: 16px;
    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.05);
}

.dark .result-item {
    background: #2d2d2d !important;
    border-color: var(--border-color-dark) !important;
    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.2) !important;
}

/* Markdown content */
.gr-markdown {
    color: var(--text-color-light);
    font-family: 'Inter', sans-serif;
    line-height: 1.6;
}

.dark .gr-markdown {
    color: var(--text-color-dark) !important;
}

.dark .gr-markdown * {
    color: var(--text-color-dark) !important;
}

/* Image containers */
.image-container {
    display: flex;
    gap: 12px;
    justify-content: center;
    margin: 12px 0;
}

.image-item {
    flex: 1;
    max-width: 300px;
}

/* Sliders */
.gr-slider {
    color: var(--text-color-light);
    font-family: 'Inter', sans-serif;
}

.dark .gr-slider {
    color: var(--text-color-dark) !important;
}

/* Accordion */
.gr-accordion {
    background: var(--bg-color-light);
    border: 1px solid var(--border-color-light);
    border-radius: 6px;
    font-family: 'Inter', sans-serif;
}

.dark .gr-accordion {
    background: #2d2d2d !important;
    border-color: var(--border-color-dark) !important;
}

/* Cleanup styles */
.reduced-text {
    font-size: 14px;
    line-height: 1.5;
    font-family: 'Inter', sans-serif;
    color: var(--secondary-color-light);
}

.dark .reduced-text {
    color: var(--secondary-color-dark) !important;
}

.compact-section {
    margin: 8px 0;
    padding: 12px;
}

.chatbot {
    background: var(--bg-color-light);
    border: 1px solid var(--border-color-light);
    border-radius: 8px;
    font-family: 'Inter', sans-serif;
    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.05);
}

.dark .chatbot {
    background: #2d2d2d !important;
    border-color: var(--border-color-dark) !important;
    box-shadow: 0 1px 3px rgba(0, 0, 0, 0.2) !important;
}

/* Chat message styling */
.message {
    background: var(--bg-color-light);
    color: var(--text-color-light);
    border-radius: 6px;
    margin: 4px 0;
    font-family: 'Inter', sans-serif;
    font-size: 14px;
}

.dark .message {
    background: #404040 !important;
    color: var(--text-color-dark) !important;
}

/* User message */
.message.user {
    background: var(--primary-color-light);
    color: white;
}

.dark .message.user {
    background: var(--primary-color-dark) !important;
}

/* Bot message */
.message.bot {
    background: #f8f9fa;
    color: var(--text-color-light);
}

.dark .message.bot {
    background: #3d3d3d !important;
    color: var(--text-color-dark) !important;
}

/* Chat input area */
.chat-input-area {
    background: var(--bg-color-light);
    border-top: 1px solid var(--border-color-light);
    padding: 12px;
}

.dark .chat-input-area {
    background: #2d2d2d !important;
    border-color: var(--border-color-dark) !important;
}

/* Study tips panel */
.study-tips {
    background: rgba(0, 123, 255, 0.04);
    border: 1px solid rgba(0, 123, 255, 0.15);
    border-radius: 6px;
    padding: 12px;
    margin: 8px 0;
    font-family: 'Inter', sans-serif;
}

.dark .study-tips {
    background: rgba(13, 110, 253, 0.08) !important;
    border-color: rgba(13, 110, 253, 0.25) !important;
}

/* Enhanced spacing and typography */
h1, h2, h3, h4, h5, h6 {
    font-family: 'Inter', sans-serif;
    font-weight: 600;
    letter-spacing: -0.01em;
    margin-bottom: 0.5em;
}

p {
    font-family: 'Inter', sans-serif;
    line-height: 1.6;
    margin-bottom: 1em;
}

/* Subtle animations */
.gr-button, .tab-nav button {
    transition: all 0.2s cubic-bezier(0.4, 0, 0.2, 1);
}

.result-item, .gr-panel, .gr-box, .gr-form, .chatbot {
    transition: box-shadow 0.2s ease;
}

.result-item:hover, .gr-panel:hover, .gr-box:hover, .gr-form:hover {
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.08);
}

.dark .result-item:hover, .dark .gr-panel:hover, .dark .gr-box:hover, .dark .gr-form:hover {
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3) !important;
}

.youtube-input-section {
    background: linear-gradient(45deg, #ff000015, #ff000008);
    border: 1px solid #ff000025;
    border-radius: 8px;
    padding: 16px;
    margin: 12px 0;
}

.dark .youtube-input-section {
    background: linear-gradient(45deg, #ff000020, #ff000010) !important;
    border-color: #ff000040 !important;
}

/* Input option sections */
.input-option-section {
    background: rgba(0, 123, 255, 0.02);
    border: 1px solid rgba(0, 123, 255, 0.1);
    border-radius: 6px;
    padding: 12px;
    margin: 8px 0;
}

.dark .input-option-section {
    background: rgba(13, 110, 253, 0.05) !important;
    border-color: rgba(13, 110, 253, 0.15) !important;
}

/* Section headers */
.section-header {
    color: var(--primary-color-light);
    font-weight: 600;
    font-size: 1.1em;
    margin-bottom: 8px;
    font-family: 'Inter', sans-serif;
}

.dark .section-header {
    color: var(--primary-color-dark) !important;
}

/* URL validation indicators */
.url-valid {
    border-color: #28a745 !important;
    box-shadow: 0 0 0 2px rgba(40, 167, 69, 0.1) !important;
}

.url-invalid {
    border-color: #dc3545 !important;
    box-shadow: 0 0 0 2px rgba(220, 53, 69, 0.1) !important;
}

/* Enhanced video section */
.video-upload-section {
    border: 2px dashed var(--border-color-light);
    border-radius: 8px;
    padding: 16px;
    text-align: center;
    transition: all 0.3s ease;
    background: rgba(0, 123, 255, 0.01);
}

.video-upload-section:hover {
    border-color: var(--primary-color-light);
    background: rgba(0, 123, 255, 0.03);
}

.dark .video-upload-section {
    border-color: var(--border-color-dark) !important;
    background: rgba(13, 110, 253, 0.03) !important;
}

.dark .video-upload-section:hover {
    border-color: var(--primary-color-dark) !important;
    background: rgba(13, 110, 253, 0.06) !important;
}

/* Processing status indicators */
.processing-status {
    background: rgba(0, 123, 255, 0.1);
    border: 1px solid rgba(0, 123, 255, 0.2);
    border-radius: 6px;
    padding: 8px 12px;
    margin: 4px 0;
    font-size: 14px;
    font-family: 'Inter', sans-serif;
    color: var(--primary-color-light);
}

.dark .processing-status {
    background: rgba(13, 110, 253, 0.15) !important;
    border-color: rgba(13, 110, 253, 0.3) !important;
    color: var(--primary-color-dark) !important;
}

/* Success/Error message styling */
.status-success {
    background: rgba(40, 167, 69, 0.1);
    border-color: rgba(40, 167, 69, 0.2);
    color: #155724;
}

.status-error {
    background: rgba(220, 53, 69, 0.1);
    border-color: rgba(220, 53, 69, 0.2);
    color: #721c24;
}

.dark .status-success {
    background: rgba(40, 167, 69, 0.15) !important;
    border-color: rgba(40, 167, 69, 0.3) !important;
    color: #28a745 !important;
}

.dark .status-error {
    background: rgba(220, 53, 69, 0.15) !important;
    border-color: rgba(220, 53, 69, 0.3) !important;
    color: #dc3545 !important;
}

/* Enhanced tip section */
.tip-section {
    background: linear-gradient(135deg, rgba(255, 193, 7, 0.1), rgba(255, 193, 7, 0.05));
    border: 1px solid rgba(255, 193, 7, 0.2);
    border-radius: 6px;
    padding: 10px 12px;
    margin: 8px 0;
    font-size: 13px;
    font-family: 'Inter', sans-serif;
}

.dark .tip-section {
    background: linear-gradient(135deg, rgba(255, 193, 7, 0.15), rgba(255, 193, 7, 0.08)) !important;
    border-color: rgba(255, 193, 7, 0.3) !important;
}

/* Input group styling */
.input-group {
    display: flex;
    flex-direction: column;
    gap: 12px;
    margin: 16px 0;
}

.input-group-header {
    font-weight: 500;
    color: var(--text-color-light);
    margin-bottom: 4px;
    font-family: 'Inter', sans-serif;
}

.dark .input-group-header {
    color: var(--text-color-dark) !important;
}

/* Progress bar enhancements */
.progress-container {
    background: rgba(0, 123, 255, 0.05);
    border-radius: 4px;
    padding: 8px;
    margin: 8px 0;
}

.dark .progress-container {
    background: rgba(13, 110, 253, 0.08) !important;
}

/* Enhanced button hover effects */
.gr-button:not(:disabled):hover {
    transform: translateY(-1px);
    box-shadow: 0 4px 12px rgba(0, 123, 255, 0.25);
}

.dark .gr-button:not(:disabled):hover {
    box-shadow: 0 4px 12px rgba(13, 110, 253, 0.3) !important;
}

/* Responsive adjustments */
@media (max-width: 768px) {
    .input-group {
        gap: 8px;
    }

    .video-upload-section {
        padding: 12px;
    }

    .section-header {
        font-size: 1em;
    }
}
"""

with gr.Blocks(css=custom_css, title="AI Classroom Assistant", theme=gr.themes.Soft()) as demo:
    gr.HTML('<h1 class="main-header">AI Classroom Assistant</h1>')

    with gr.Tab("Lecture Processor"):
        gr.Markdown(
            "Upload a lecture video or paste a YouTube URL to get transcript, notes, images, and resources",
            elem_classes=["reduced-text"]
        )

        with gr.Row():
            with gr.Column():
                gr.Markdown("### Option 1: Upload Video File")
                pipeline_video_input = gr.Video(
                    label="Upload Lecture Video",
                    height=300,
                    elem_classes=["video-section"]
                )

                gr.Markdown("### Option 2: YouTube URL")
                pipeline_youtube_input = gr.Textbox(
                    label="YouTube Video URL",
                    placeholder="https://www.youtube.com/watch?v=... or https://youtu.be/...",
                    lines=1,
                    elem_classes=["result-item"]
                )


        with gr.Row():
            with gr.Column(scale=1):
                with gr.Accordion("Settings", open=False):
                    pipeline_search_results = gr.Slider(5, 15, value=8, label="Search Results")
                    pipeline_num_images = gr.Slider(1, 4, value=2, label="Images")
                pipeline_btn = gr.Button("Process Lecture", variant="primary", size="lg")

            with gr.Column(scale=2):
                gr.Markdown("", elem_classes=["reduced-text"])

        with gr.Row():
            with gr.Column():
                pipeline_transcript = gr.Textbox(
                    label="Transcript",
                    lines=8,
                    show_copy_button=True,
                    elem_classes=["result-item"]
                )
                pipeline_notes = gr.Textbox(
                    label="Structured Notes",
                    lines=8,
                    show_copy_button=True,
                    elem_classes=["result-item"]
                )

            with gr.Column():
                with gr.Row():
                    pipeline_image1 = gr.Image(label="Related Image 1", height=180)
                    pipeline_image2 = gr.Image(label="Related Image 2", height=180)
                pipeline_search = gr.Textbox(
                    label="Web Resources",
                    lines=8,
                    show_copy_button=True,
                    elem_classes=["result-item"]
                )

        pipeline_btn.click(
            process_youtube_or_upload_robust,
            inputs=[
                pipeline_video_input,
                pipeline_youtube_input,
                pipeline_search_results,
                pipeline_num_images
            ],
            outputs=[
                pipeline_transcript,
                pipeline_notes,
                pipeline_image1,
                pipeline_image2,
                pipeline_search
            ],
            show_progress=True
        )

    with gr.Tab("Transcription"):
        gr.Markdown("Convert video speech to text", elem_classes=["reduced-text"])
        with gr.Row():
            with gr.Column():
                video_input = gr.Video(label="Upload Video", height=250)
                trans_btn = gr.Button("Transcribe", variant="primary")
            with gr.Column():
                transcription_out = gr.Textbox(label="Transcript", lines=10, show_copy_button=True)
        trans_btn.click(transcribe_audio_from_video, inputs=video_input, outputs=transcription_out, show_progress=True)



    with gr.Tab("Notes"):
        gr.Markdown("Generate structured notes from text", elem_classes=["reduced-text"])
        with gr.Row():
            with gr.Column():
                notes_transcript = gr.Textbox(label="Input Text", lines=8)
                notes_btn = gr.Button("Generate Notes", variant="primary")
            with gr.Column():
                notes_output = gr.Textbox(label="Structured Notes", lines=10, show_copy_button=True)

        notes_btn.click(generate_structured_notes, inputs=[notes_transcript], outputs=notes_output, show_progress=True)

    with gr.Tab("Search"):
        gr.Markdown("Search web and videos", elem_classes=["reduced-text"])
        with gr.Row():
            with gr.Column():
                search_query = gr.Textbox(label="Search Query")
                with gr.Row():
                    search_results_num = gr.Slider(3, 15, value=8, label="Results")
                    include_videos_check = gr.Checkbox(label="Include Videos", value=True)
                search_btn = gr.Button("Search", variant="primary")
            with gr.Column():
                search_output = gr.Textbox(label="Results", lines=10, show_copy_button=True)

        search_btn.click(
            search_web_enhanced,
            inputs=[search_query, search_results_num, include_videos_check],
            outputs=search_output,
            show_progress=True
        )

    with gr.Tab("Images"):
        gr.Markdown("Search for relevant images", elem_classes=["reduced-text"])
        with gr.Row():
            with gr.Column():
                image_search_query = gr.Textbox(label="Image Search Query")
                image_num_slider = gr.Slider(1, 6, value=2, label="Number of Images")
                image_search_btn = gr.Button("Search Images", variant="primary")
            with gr.Column():
                with gr.Row():
                    image_output1 = gr.Image(label="Image 1", height=200)
                    image_output2 = gr.Image(label="Image 2", height=200)
                with gr.Row():
                    image_output3 = gr.Image(label="Image 3", height=200)
                    image_output4 = gr.Image(label="Image 4", height=200)

        def search_and_display_images(query, num_images):
            if not query.strip():
                return [None] * 4

            image_urls = search_google_images(query, min(num_images, 4))
            images = []
            for url in image_urls:
                img = download_image_from_url(url)
                images.append(img)

            while len(images) < 4:
                images.append(None)

            return images[:4]

        image_search_btn.click(
            search_and_display_images,
            inputs=[image_search_query, image_num_slider],
            outputs=[image_output1, image_output2, image_output3, image_output4],
            show_progress=True
        )

    with gr.Tab("Study Chat"):
        gr.Markdown("Ask questions about your studies", elem_classes=["reduced-text"])

        with gr.Row():
            with gr.Column(scale=4):
                chatbot_interface = gr.Chatbot(
                    label="AI Study Assistant",
                    height=500,
                    show_copy_button=True,
                    bubble_full_width=False,
                    elem_classes=["result-item"]
                )

                with gr.Row():
                    with gr.Column(scale=4):
                        chat_input = gr.Textbox(
                            label="Ask a question",
                            placeholder="Ask me anything about your studies - concepts, definitions, explanations...",
                            lines=2,
                            max_lines=4
                        )
                    with gr.Column(scale=1, min_width=100):
                        chat_send_btn = gr.Button("Send", variant="primary", size="sm")

                with gr.Row():
                    clear_chat_btn = gr.Button("Clear Chat", variant="secondary", size="sm")

        def handle_chat_submit(message, history):
            return generate_chatbot_response(message, history), ""

        chat_input.submit(
            handle_chat_submit,
            inputs=[chat_input, chatbot_interface],
            outputs=[chatbot_interface, chat_input]
        )

        chat_send_btn.click(
            handle_chat_submit,
            inputs=[chat_input, chatbot_interface],
            outputs=[chatbot_interface, chat_input]
        )

        clear_chat_btn.click(
            clear_chat_history,
            outputs=[chatbot_interface]
        )

    gr.HTML('<div style="text-align: center; margin-top: 30px; padding: 20px; background: rgba(0,123,255,0.1); border-radius: 10px; font-family: Inter, sans-serif;"><h3>Made by Kewal Thacker and Siddharth Subramanian</h3></div>')

demo.launch(share=True, debug=True)